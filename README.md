# 使用tnesorflow進行的深度學習練習

## Part1 初入深度學習
參考[官方文檔](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb)中的範例，進行深度學習練習，分為幾個主要步驟

1. 載入資料
2. 資料預處理
3. 設計/編譯模型
4. 訓練模型
5. 測試模型
6. 模型的改進

這些內容包含在文檔中，可以知道以下內容
1. `keras.utils`有多種載入資料集的方法
2. `batch` 指的是一次包含多少資料，因此使用`.shape`時會得到`batch數x單一物件大小`
3. 模型建立以`model = Sequential([...])`來操作，這裡也可以進行一部分的預處理
4. `tensorflow` 以 先編譯模型，再進行模型訓練 為主要方法，因此較為快速
5. `Dataset.cache`可以讓資料建立快取使資料IO不是最大限制
6. `model`物件有多種方式調查資料，例如`.summary()`可得知模型資訊`.fit()`方法可以訓練並且返回一個物件紀錄`fit`結果，這個過程中可加入`validation_data`來衡量結果，history可以分別調用不同資料集的準確度和loss等資訊，可以透過這些資訊簡單畫圖
7. 當訓練次數上去反而使的驗證資料集效果下降，通常代表overfitting，這通常是由於資料數量太少，但是透過增強原本的資料通常會是更好的解，例如旋轉圖像、增加雜訊、放大縮小、整體變明/暗等
8. 這樣的操作可以用一個`Sequential`方法建立，因此他同樣是一個模型而這樣的模型可以作為一層layer插入在原模型上
9. `tflite`可以簡單的下載模型，該案例中產生的模型約為15MB
## 嘗試
1. 修改為灰階圖片

灰階圖片比RGB圖片更加的簡單，且資料大小更小

在練習中除了資料名稱以外主要是由於資料格式變化了，每個batch從(32, 180, 180, 3)變為(32, 180, 180, 3)

因此需要修改Rescaling的input shape，但是除此之外的部分幾乎不用修改，可以看出神經網路對於資料輸入限制很小，並且這樣的執行直到fit時才出現問題，這是由於compile不涉及資料，並且錯誤題是會明確的知會是哪一層的問題

另一個出問題的地方在`RandomFlip`

2. 製作自定義層

## 我發現的
1. `.prefetch()`後返回的資料集與原本的資料類型不同，它是`_PrefetchDataset`對象
2. 灰階與彩色圖的loss曲線相似，但是數值有差異，彩色圖的loss更低，我認為這代表主要的訓練結果是在形狀上的訓練，而顏色的影響則並沒有隨著訓練次數增加而提升，或者應該說，顏色的部分更早的學習完畢
3. 先compile model再來製作資料也是可行的
4. 常見的處理，例如縮放、標準化、正規化、翻轉旋轉等都已經被設置好了layer，直接使用他們比在資料集操作更快
5. 自定義layer的製作應當是可能的
